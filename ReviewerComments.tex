\documentclass[12pt]{article}
\usepackage[margin=1.0in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{natbib}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\usepackage[margin=1cm]{caption}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{xr}

\renewenvironment{quote}  % Define 
              {\list{}{\rightmargin\leftmargin}\normalfont%
               \item\relax}
              {\endlist}


\externaldocument[M-]{Draft}

%\doublespacing

\newenvironment{itquote}
{\begin{quote}\itshape}
{\end{quote}}

\newcommand{\adam}[1]{{\color{blue} ADAM: #1}}
\newcommand{\jarad}[1]{{\color{Orange} JARAD: #1}}

\newenvironment{indpar}[1]%
     {\begin{list}{}%
             {\setlength{\leftmargin}{\#1}}%
             \item[]%
     }
     {\end{list}}

\newcommand{\vn}{\textbf{n}}
\newcommand{\vp}{\textbf{p}}
\newcommand{\vX}{\textbf{X}}
\newcommand{\vZ}{\textbf{Z}}
\newcommand{\vbeta}{\boldsymbol{\beta}}
\newcommand{\vxi}{\boldsymbol{\xi}}

\newcommand{\Exp}{\mbox{Exp}}
\newcommand{\Ga}{\mbox{Ga}}
\newcommand{\We}{\mbox{We}}
\newcommand{\LN}{\mbox{LN}}
\newcommand{\Po}{\mbox{Po}}
\newcommand{\Mult}{\mbox{Mult}}

\newcommand{\pdet}{p^{(det)}}
\newcommand{\ind}{\stackrel{ind}{\sim}}
\newcommand{\Fm}{F_T^{(M)}}

\begin{document}
\itshape
Associate Editor:

I enjoyed reading this well-written manuscript, and believe that the modeling framework proposed is sufficiently versatile to be used in a wide range of applications. In addition to the points made by the referees, I have two comments:
\begin{enumerate}
\item Reviewer 1 is not convinced that it is helpful to make explicit the link to survival analysis. I tend to disagree with this, since I think that this is what renders the approach widely applicable and also extendable. In fact, the general idea strongly reminded me of several papers published recently by David Borchers and colleagues (using concepts from survival analysis to model detection events within distance sampling and spatially explicit capture-recapture). I was wondering if there is a common denominator between his work and the present manuscript (but the authors need not necessarily explore this).
\begin{quote}
We agree, this manuscript has strong parallels to the continuous-time models of David Borchers and colleagues.  
Interestingly, they have approached removal modeling both from distributional and from hidden Markov modeling directions.
Our manuscript explores distinct topics in that our model is for removal-only data, whereas their models are for removal-distance data.
Also, we specifically investigate non-constant detection distributions, which has not been their focus.
\end{quote}


\item In Section 3.4, models are built where the various TTDDs considered are linked to covariates via the rate parameter. It was not clear to me why the rate parameter was chosen to be modeled as a function of covariates, as opposed to the mean of the TTDD. For example, in gamma GLMs, the mean is modeled as a function of covariates, with the shape parameter fixed (the latter as done also in the present work). Basically I'm now wondering a) why the rate rather than the mean was modeled, and b) if that matters at all. In 3.1, a justification seems to be given, but I do not follow the reasoning here â€“ why would the inclusion of random and fixed effects be any different if not the rate but the mean would be used? Sorry if I'm just missing the obvious.
\begin{quote}
We have added the following in Section \ref{M-sec:exact_time} to clarify: ``We employ a log link to model $\varphi$, and therefore our model is equivalent to a generalized linear model with a log link on the mean detection time."

We feel the rate conceptualization resonates more with the framing of the problem, though perhaps the distinction is mainly aesthetic.
Our view might be more explicit if we were to define the time-to-detection distribution from its hazard function rather than from a traditional distributional family. 
The latter is more convenient and builds upon parametric survival modeling, but the former can also be done within our model framework.
\end{quote}
\end{enumerate}

\newpage



Reviewer \#1:

I have included several items for the authors to consider below and provide this review as an Ecologist and potential model user.
Several pieces of information are lacking from the manuscript that will be relevant to users of this method.
\begin{enumerate}
\item What types of data are relevant to each examined distribution (e.g., gamma, Weibull)?
\item What sample sizes (i.e., number of detections) are needed to obtain reliable estimates with CIs that are not so wide as to be meaningless?
\item What are approximate computation times for models of different complexity (e.g., days? weeks?).
\end{enumerate}
\begin{quote}
We address each of these in turn as they appear in the comments below. 
\jarad{Answer them here even if it repeats information below.}
\end{quote}


Abstract

Line 14: Perhaps instead of saying its not justified, mention it can lead to bias
\begin{quote}
We have revised the statement to read: ``The standard model for estimating detection from removal-sampled point-count surveys assumes that organisms at a survey site are detected at a constant rate; however, this assumption can often lead to biased estimates.''
\end{quote}

Line 29-31: Clarify that you conducted a simulation analysis and your results from that analysis suggest it can outperform non-mixture models.
\begin{quote}
We have prefaced the statement with the clause: ``Based on simulation analyses, ..."
\end{quote}

Line 40-41.  Also worth mentioning caveats - e.g., greater imprecision and computation times.
\begin{quote}
\adam{I certainly don't intend to address either of these in the Abstract.  Both are discussed in the text.  Computation times are not addressed in the Discussion.  Let me know if you think otherwise.}
\jarad{I think that's fine. In responding to the review, we can state our 
opinion that the abstract is not the appropriate place and point the reviewer
to where the content can be found.} 
\end{quote}

Introduction\\
Line 36-41: I find mention of survival analyses here and later in the manuscript confusing for most readers - especially Ecologists, presuming this is the intended audience.  It may be due to the unorthodox use of time-to-detection instead of traditional time-to-event analyses often used to estimate survival.  I suggest either omitting the reference to survival analysis here and elsewhere in the manuscript, or using the time-to-event terminology in this sentence and providing citations for the cdf and pdfs outlined here for reference.  Then explain that within the manuscript you will subsequently refer to it as TTDD.  
\begin{quote}
We have made the equivalence more explicit: ``We analyze times to first detection as time-to-event data, as is done in parametric survival analysis, ..."
\jarad{We probably need to add more, e.g. intended audience. Philip?} 
\end{quote}

3.1 \\
Line 60: In the context of estimating abundance of wildlife, justifying the inclusion of the Weibull and lognormal distributions because they are often used in survival analysis is weak.  Are there biologically plausible scenarios in which these distributions are appropriate given this type of data?  Justify here similar to the Gamma distribution.  Otherwise, omit.
\begin{quote}
We have added the following sentences to \ref{M-sec:exact_time}: ``Like the gamma TTDD, Weibull and lognormal TTDDs offer the flexibility of a two-parameter form and allow rates to increase or decrease during the survey.
All three TTDDs may provide reasonable empirical approximations of non-constant detection, though the shapes of the distributions differ, potentially leading to differing inference.
For instance, when detection rates vary across individuals, the result is a marginal detection rate that decreases over time.
Whether the marginal rate is best approximated by a gamma, lognormal, or Weibull TTDD depends on just how rates vary across individuals."

We appreciate the importance of providing biological justification for statistical assumptions.
It is always a good foundation.
That being said, the use of any time-to-detection distribution is a statistical approximation to empirical patterns.
If the dynamics of the system are more complicated than we can directly model, the best we can do is find a TTDD that reasonably describes the observed pattern.
For instance, if the arrival of the observer causes non-constant detection, we know neither how long that observer effects lasts nor how the degree of that effect changes over time.
Gamma, lognormal, and Weibull TTDDs all provide different models for that detection pattern with different inferences detection probability.
\end{quote}

Simulation Analyses\\
Line 57-59: This is the first mention of peaked and non-peaked TTDDs.  Please define these terms and how they are manifested during point-count surveys in the Introduction - including a biological justification that warrants their inclusion in simulation studies.
\begin{quote}
We have given the definitions of peaked/nonpeaked their own paragraph in the Simulation Studies methodology (Section \ref{M-sec:sim}):
``In the following analyses we distinguish two categories of purely continuous TTDDs: peaked and nonpeaked.  
Detection rates $h(t)$ of peaked distributions generally increase over time, while detection rates of nonpeaked distributions generally decrease over time.
More formally, we define a peaked TTDD as having a mode greater than zero (or $C_1$ for lognormal) while a non-peaked TTDD has a mode of zero (or less than $C_1$), but we consider exponential TTDDs to be neither peaked nor nonpeaked."

\jarad{We should probably remove TTDD from the introduction.}

We do not believe this distinction requires a separate explanation in the Introduction.  It distinguishes between scenarios but is not essential to framing the problem of non-constant detection.
\end{quote}

As a potential user of such a model presented in this manuscript, I find the simulation analyses somewhat lacking.   
\begin{enumerate}
\item Users will most certainly not bother using such complex modeling approach to model an intercept only - and thus, evaluating TTDDs based on intercept-only models seems overly simple and not very informative. \begin{quote}
We agree that an intercept-only model is simplistic and will not be of much practical use in an applied setting.
However, the intercept-only simulation is informative all the same, because models that perform poorly in simple applications can hardly be trusted in more complex applications.

Meanwhile, as discussed later, a simulation study involving effects on abundance and detection is computationally expensive.
Fortunately, while simulation studies are hampered by long computation times, the typical user of this model will only apply it to a small number of datasets, so computation time will not be prohibitive.
\end{quote}

\item Simulations incorporating variable sample sizes to evaluate relative bias and precision would be very useful in terms of study design and applicability.  
\begin{quote}
\jarad{I think we can thank reviewers here for the push to do more simulation 
studies that resulted in our realization of a computational savings allowing
us to do more analyses.}
We have extended our simulation study to include a variety of detection probabilities rather than a variety of sample sizes.
Both affect estimation and are worthy of investigation, but since the performance of unknown-$N$ binomial models generally deteriorates as values of $\pdet$ decrease, we chose to direct our attentions there.
\jarad{Don't we have different sample sizes due to the variation in detection
probabilities?}
Sections \ref{M-sec:mixture} and \ref{M-sec:family} have been reworked to detail the updated findings.
\end{quote}

\item How might study design impact bias and precision?  For example, length of survey relative to peaked TTDDs and pooling of time periods during data collection?  These are also scenarios that would benefit readers.  Alternatively, these issues could be addressed in the Discussion.
\begin{quote}
These are important decision in the design of surveys, but they are beyond the scope we have defined for this study.
Not having explicitly investigated them, we prefer not to speculate.
%\citet{Petit1995, Johnson2008, LeeMarsden2008} and \citet{Reidy2011} all address the effects of duration.
We have separated out a paragraph in the Discussion posing some design-related questions.
\jarad{Be more specific.}
\end{quote}

\item Providing computation times for scenarios that vary by model complexity and sample size (including the computer used for analyses) may also provide a gauge for users as to whether this method is feasible for their needs.
\begin{quote}
Average run times for the different simulation studies are now discussed in the third paragraph of Section \ref{M-sec:simfull}:

``Computation times for this simulation study were much greater than for the other studies because partitions of the cdf, e.g. $\Fm(C_i|\alpha,\varphi_s)$, had to be calculated separately for every survey; also, sampling often required 2-8 as many iterations.
Average times for exponential, lognormal, Weibull, and gamma model fits were 2.6, 5.1, 5.3, and 25+ hours, respectively, as compared to only 2, 3, 3.25, and 5.35 minutes for the intercept-only models.
\jarad{Too many significant digits.}
Due to the computation times involved, we fit each model only once."
\end{quote}

\item I donâ€™t fully appreciate the need for both 50\% and 90\% coverage estimates.  It seems one estimate would be sufficient, and typically 90\% or 95\% credible intervals are used with these types of analyses?
\begin{quote}
We agree that using both is redundant.
We now choose to report only 90\% credible intervals.
\jarad{Are 50\% included in supplement?}
\end{quote}
\end{enumerate}

Results\\
Page 18, Figure 2:  Holy moly, the CIs of the mixture distributions are exceedingly wide given a sample size of 381 detections (which is a lot relative to many studies).
\begin{quote}
Clarification: We have 947 detections at 381 sites (which is \textit{really} a lot of detections).
We now clearly state in Section \ref{M-sec:data}: ``This dataset includes 947 Ovenbirds counted in 381 surveys at 65 sites with site-specific variables including site age,  stock density, and an indicator of select-/partial-cut logging during the 1990s."

We believe this uncertainty accurately reflects the uncertainty in estimation
of detection probability and abundance. 
The apparent certainty in previously published studies relies heavily on the 
assumption of constant detection rate.
\end{quote}

For example, The Gamma distribution, touted as the most accurate based on the simulation study, suggests 95\% CIs of p ranges from ~ 0.3 to 1?  And subsequently CIs around N are from ~1.75 to 3.25 so we can say there are somewhere between 56 and 1,778  birds?
Is that a useful estimate?  There is a tradeoff between accuracy and precision happening here, and although precise, biased estimates are not what we want, any alternative needs to provide precision at levels that are still informative.  This is a limitation not sufficiently addressed in the Discussion.
\begin{quote} 
We think our figure was unclear.
The cited credible intervals were for \textit{uncounted} birds not total abundance.
We have now changed the figure to $\log_{10}$(total abundance) to avoid confusion.
The gamma TTDD 95\% credible interval for total abundance is (1007, 2836).
This two- to three-fold scale of uncertainty is commensurate with other studies cited in this manuscript \citep{Diefenbach2007, Reidy2011, Solymos2013, Amundson2014}.
\end{quote}

Discussion\\
Page 21, Lines 12-16:  I do not agree with this assessment.  I think these models are appropriate for a particular subset of data - single species discrete-count surveys with a large sample size and relatively low availability (i.e., whether the animal provides a cue to an observer during the survey)   
\begin{quote}
We hope that clarifications above about abundance estimates and uncertainty resolve some of the reviewer's hesitations.
We have added single-species' to the draft.

\adam{Perhaps more explanation is required here.}
\end{quote}
where non-constant detection through time is suspected based on the behavioral ecology of the species.
\begin{quote}
This last clause implies that a constant-rate model should be the default.
We disagree.
Based on our results and applying the precautionary principle, we believe the burden of proof should rest with the analyst who wishes to assume constant detection.
This is a central point in our manuscript: the constant-rate assumption should not be the default.
\end{quote}

As the authorsâ€™ suggest, most point-count studies are designed to maximize availability, which is typically high for birds (e.g., 80-95\%) with perceptibility (e.g., the probability an observer detects a cue that is given by an animal) being the much larger source of bias.  Therefore, this model, without including a supplemental method to estimate perceptibility, is of limited use to many readers.
\begin{quote}
We agree.
This manuscript applies to removal-only analyses.
Still, the concept can be carried over to removal-distance analyses, as we describe in a paragraph of the Discussion.
The implementation will be more complicated and will vary depending on how removal- and distance-sampling are integrated.
\jarad{This is ongoing work for us.}
\end{quote}
Further, although the method may be more accurate even for constant detection data, the loss in precision is such that users will likely not opt to use it unless they suspect non-constant detection through time.  Further, given its presumably arduous computation times (albeit unknown), adding a perceptibility component to the model would likely greatly increase computational demands with unknown implications to bias and precision.  Including a paragraph in the Discussion that outlines when the model would be most beneficial to users (e.g., what type of data, surveys, or sample sizes) and explicitly stating drawbacks (e.g., computation time, loss of precision) would greatly improve this section.  
\begin{quote}
\adam{I do not deem this necessary.  We have not explicitly studied sample size.  The rest is addressed at various locations in the manuscript but does not, I think, require a dedication paragraph in the Discussion.}
\jarad{I'll have to re-read, but overall providing a summary for the reader
seems like a good idea.}
\end{quote}

\newpage



Reviewer \#2:

Although many statistical solutions to known issues in abundance estimation have been proposed to date, a plethora of relevant problems remain untreated.  In this manuscript, the author(s?) put the finger on the wound, so to speak, and deal with a very relevant problem: allowing for heterogeneity in detection, or a non-constant detection rate during animal point-count surveys.  This topic is relevant because, as the authors clearly state multiple times, the statistical properties of abundance estimates are sensitive to the statistical properties of the estimates of the detection probabilities. In that sense, I commend the manuscript's pertinence: the question treated is indeed relevant and needs careful examination.
The advent of computer intensive approaches to estimate parameters in hierarchical models very quickly re-shaped the field of abundance estimation, and in less than a decade, we've moved from trying to explain a complicated natural signal with the simplest possible model to trying to explain a complicated natural signal with a `realistic' but equally complicated sampling model. Then, the burden of the quality of the estimation is put into the process of the specification of the sampling model. However, there is little guarantee that the data, as considered in the paper, contains the necessary information to be able to reliably tease apart all the components of the statistical sampling model. 
\begin{quote}
We appreciate the reviewers' insights and whole-heartedly agree.
\end{quote}
In what follows, I not only expand on this topic but I outline a few major and minor comments that I hope the author(s) will regard as useful to improve the quality of the manuscript. I will certainly recommend this manuscript for publication after the author(s) include the
modifications I request or she (he) successfully convinces me otherwise.



1. MAJOR COMMENTS


Diagnostics: Coverage. The idea of testing coverage is fantastic, but poorly implemented. First, the number of simulations (16 in Table 1) is exceedingly low to be able to reliably diagnose the patterns in coverage or the statistical explanation of these patterns. 
\begin{quote}
We answer this comments in two parts.
First, we agree that the number of simulations was inadequate for diagnosing model accuracy.
We have therefore increased to 100 the number of simulations in Sections \ref{M-sec:mixture} and \ref{M-sec:family}.
This has been made possible by improvements to Stan \citep{Rstan2016} and by streamlining our code for homogeneous surveys.
See the second paragraph of Section \ref{M-sec:simfull} for a summary of run times for the models currently.

Second, although the original number of simulations was inadequate for characterizing model accuracy, it nonetheless highlighted scenarios of gross inaccuracy, such as: (i) omitting a mixture component in modeling a peaked mixture dataset, or (ii) assuming constant detection when the rate varies during the survey period.
\jarad{Is the second point necessary?}
\end{quote}

Second, the statistical properties of the estimator of p(det) are likely depending on the size of the true value of p(det). Therefore, I think Table 1 should have been repeated for a whole range of values of the true value of p(det).
(Is a boundary like the one in Olkin et al 1981, for your case, a hard one such that below it estimation is bad, and above it estimation is ``uniformly" good?).
Without variation in ``true" simulated scenarios, it is easy to inadvertendly ``stack the deck" in favor or against a particular combination of settings (Particularly since your ``true" value of p is high: 0.8). A wide range of simulated truths is also necessary because to be useful, these methods should be widely applicable in the
tropics (low N's, low p's very often) as well as in temperate forests (large N's, large p's), for example.
\begin{quote}
We have repeated the simulations in Sections \ref{M-sec:mixture} and \ref{M-sec:family} at true detection probabilities $\pdet = 0.50, 0.65, 0.80$, and 0.95.
The findings have significantly added to our results and conclusions.
\jarad{Should we summarize the results here?}

With reference to Olkin's finding that point estimates of $N_s$ are unstable unless $\pdet>0.41$, it is not a hard boundary.  
It varies by context, but the boundary is a general property of estimating $N$ in binomial unknown-$N$, unknown-$p$ problems.
Olkin's calculation derives from the relative sample mean and sample variance of repeated large samples with the same values of $N$ and $\pdet$.
As such, it is a best-case scenario.
Similar thresholds continue to recur in various abundance-estimating contexts, as demonstrated in these recent articles: 
\begin{itemize}
\item \citet{Veech2016}: advise caution when $p<0.5$ in the context of individual-level detection effects (no time-to-detection data). 
\item \citet{Field2016}: plots indicate uncertainty in $\hat{N}$ increase rapidly as $\pdet$ decreases below 0.40.  Methods include removal, double-observer, distance, and multiple-visit.
\item \citet{Davis2016}: report accurate $\hat{N}$ at $\pdet >0.40$ in a traditional removal-sampling context, though for small abundance ($N<50$) they required $\pdet > 0.7$.
\end{itemize}
\jarad{I don't think this parameter is necessary.}
\end{quote}

Diagnostics: relation of the target parameter to other parameters in the models. Precisely because, as the authors mention, the statistical properties of the estimator of p(det) are tightly linked with the statistical properties of the abundance estimator, then the author(s) should detail explicitly how a bad coverage in p(det), for instance, affects the estimation of abundances. The author(s) mention that bias in p translates into a bias in abundance estimation, I would like to see diagnostics for the realized N values too. And to do that, two words come to mind: profile likelihoods. 
Unless you are explicitly adopting a subjective Bayesian approach, you basically declare a priori ignorance for your parameters.  Furthermore, you state that typically you will be in a case where data sets aren't large, so the information in the data is not ``swamping" the priors, so to speak.
\begin{quote}
With the expansion of our simulations to include multiple true values for $\pdet$, we have switched from tabular summaries of results to graphical summaries (though the tabular summaries will still be available in the supplemental material).
We think the most informative way to present results on the abundance scale is to add them to the Figure \ref{M-fig:sim2typical} results on detection probabilities.
This figure presents examples of entire posterior distributions rather than just median and coverage point estimates.
The inclusion of abundance estimates in Figure \ref{M-fig:sim2typical} should convey much the same information as would profile likelihoods.
\end{quote}

Lele et al 2010 (a frequent co-author of Solymos) propose a pretty neat diagnostic tool to assess
estimability that is the by product of tricking a bayesian MCMC set up into Maximum Likelihood estimation for hierarchical models (see Lele et al 2007, 2010 and others. The keyword is ``Data Cloning"). You are one step away from using Solymos and Lele's ``Data Cloning" (DC) approach to get the full ML estimation working (See Lele et al 2010 and other Data Cloning papers). Now, this is relevant because using DC you can provide clear estimability diagnostics for the parameters of interest.  Are there parameters that are technically not estimable?  Given that you don't have informative priors, you are ``driving in the dark" so to speak if you are not certain if some of your parameters are un-identifiable (see Lele et al 2010 and discussion in Lele and Dennis 2009).  Note that I am NOT asking you to re-do all the analysis using a ML approach, I am just suggesting the fact that using DC to get the ML estimates one can, as a very useful by product, get very neat diagnostics regarding
the estimability of your parameters.  Again, see Lele et al 2010.  Implementing DC would only imply a simple modification to the programs you already have. And by the way, Solymos has a DC package easy to use.
\begin{quote}
Thank you for pointing out \citet{Lele2010}.
We found the article to be interesting and novel, using data cloning to exploit relationships among data likelihood, priors, and identifiability.
While the approach could certainly be applied to our model in its more complex forms (e.g. with fixed and random effects), to do so in this manuscript would distract from our main objectives.

We agree that discussions of identifiability and the flatness of the likelihood is very relevant, and we have introduced an explicit discussion of them in the Discussion.

As an informational note, the \texttt{R} package by Solymos was constructed for BUGS and JAGS, though it would not be difficult to adapt data cloning for Stan.
\jarad{Remove paragraph.}
\end{quote}

Generalized Pareto: In another area in biology, in evolutionary genetics, the ``fitness" of individual variant strains has been modeled as coming from an exponential distribution.  For various reasons having to do with the biology of the system, such model was quickly challenged and pretty soon papers questioning the validity of this or this other model for data akin to your waiting times abounded. There is one pretty interesting paper by Beisel et al, 2007 (Genetics, 10.1534/genetics.106.068585), written towards the epiloge of such discussions, that posits that a particular parameterization of the Generalized Pareto distribution, with a single stroke, encompassed many suitable probabilistic models in the Weibull, Gumbel and Freched domain of attraction.  I wonder if the authors could write a general parameterization so that changing from one distribution to the other would simply amount to dialing a given parameter, much like Beisel et al do?  This would be just a practical
consideration that can at once deal with model selection and could speed up calculations
\begin{quote}
We appreciate the merit of the reviewer's suggestion and have added related comments in the Discussion.
We focus on a possible generalized gamma TTDD, which is an umbrella distribution that includes exponential, gamma, lognormal, and Weibull distributions as special cases.
The generalized Pareto analysis in \citet{Beisel2007} requires observations from the tail of the TTDD distribution, which we do not have.
Additionally, its focus is on flexible domains of attraction, whereas the TTDDs in our analysis are all from the Gumbel domain of attraction.

Here are the comments we have added:
``Just as the exponential TTDD is a special case of the two-parameter gamma and Weibull TTDDs, so all four TTDDs in our analysis are special cases the three-parameter generalized gamma distribution.
A generalized gamma TTDD encompasses a diversity hazard functions \citep{Cox2007}, eliminating the need to restrict analysis to lognormal, gamma, or Weibull TTDDs and the tail probabilities they imply.
However, maximum likelihood estimation of the generalized gamma has historically suffered from computational difficulties, unsatisifactory asymptotic normality at large sample sizes, and non-unique roots to the likelihood equation \citep{CoorayAnand2008, NoufailyJones2013}.
It may well be possible to implement our model with a generalized gamma TTDD, but especially when we consider the right-truncation of data from point-count surveys, we think model convergence would not be a trivial problem."
\end{quote}

Writing: The multiplicity of mathematical explicit meanings for the word ``mixture" may result confusing. The distinction between the mixture component a la Farnsworth et al and the N-mixture models (e.g. binomial counts with N~pois and p random) is clear to me, but it may not be for some of JABES' audience.  I suggest re-shaping the introduction to that effect. 
\begin{quote}
In the Introduction, we now clearly refer to a practice of modeling the TTDD ``as a mixture of two distributions --- a continuous-time distribution and a point mass for increased detection probability in the initial observation period."
We define our use of `mixture model' later in the paragraph:  ``We apply the term `mixture model' to any model with a mixture TTDD."
This is contrasted in our definition of `N-mixture model' in the next paragraph: ``a hierarchical framework for multinomial counts called an N-mixture model [\dotso] --- which is an entirely different use of `mixture' from the mixture models in the previous paragraph."
\jarad{We might not need the ``which..''.}
\end{quote}

One thing that the authors could do is to take the reader by the hand by building equation 1 bit by bit: present first the simplest version of equation 1 and what is customary to date, and then add modifications/hierarchies plus text and build the model with explicit equations and little by little arrive to the final product, which is the current equation 1. Nothing better than clear simple math aided by short explanations to present a model unambiguously.
\begin{quote}
We have split the first section of the model exposition into two sections: `Distributions for exact times to detection' and `TTDDs in an N-mixture model'.
We think this makes the flow of arguments clearer.
We have also introduced a new Equation (1) --- precursor to the old Equation (1) --- that provides the basic data model for exact-time homogeneous survey data.
This also should make the model progression more lucid.
\end{quote}

Write self-contained paragraphs, with one main idea, not two or three (and each one half developed). As a reviewer, I enjoyed verifying that the cited papers in the introduction and in the discussion were indeed meaningful (by downloading them and reading those that I was not familiar with). However, when many papers are cited, it is useful to expand in the text why the different papers are being cited.  Doing so not only clarifies your intentions, but does proper justice to the papers being referred to. It is also a useful exercise because it allows you to tell whether you have more than one clear ideas and hence, material for more than one paragraph.  For instance, I think that the paragraph in the introduction, line 34, page 3, could be expanded and/or broken into two paragraphs.  One introducing the time-to-detection as is done in survival analysis, and one presenting to the reader how data that seem not to conform to the constant-detection assumption is dealt with by
using the idea of the increased detection probability via a mixture component. These are two different ideas, hence two different paragraphs. The same approach to construct paragraphs through the text should be taken.
\begin{quote}
We have reviewed the entire manuscript with these points in mind.
We have split the cited paragraph and a few others.
\end{quote}

The authors could have crafted a beautiful and very telling graph (Figure 1) representing the multinomial intervals, the TTDD, the right censoring due to the end of the observation period, and the increased detection probability mixture component. Such figure (which is what I did by hand as I was reading the manuscript) would be a very useful guide in the reading of the paper.
\begin{quote}
We have added this as Figure \ref{M-fig:schematic}.
\jarad{Thank the reviewer for the suggestion.}
\end{quote}


Process vs. observation error: Finally, here's a perspective outside the author's topic, but within statistical ecology that might result in a markedly improvement of these ``N-mixture" techniques, along with all of its variants. I wonder if thinking a bit more in terms of modeling the biological process behind the data can result in a better estimation of the sampling variance and as a ``by-product", some novel understanding of the biology behind the data. I suppose the author(s) are familiar (or at least have seen) the multiple papers where statistical inference is done for a stochastic population dynamics model using time series of abundances while taking into account sampling error. In these settings, it is customary to deal with one observed abundance per time step. The sampling error and the ecologically-phrased variability are phrased using a hierarchical model.  When statistical inference is done for this hieararchical model, the information to be able to tease apart
the process from the observation variance lies within the structure of the temporal dependencies phrased in the model. Amazingly, with a single observation per time step, the sampling model is not ill posed as the time dependencies in the process brings about enough information in the data.  As a result, thinking of the biological process (in that case, the population dynamics model) yields better, unbiased estimates of the observation error. In your case, I cannot help but wonder if having data varying along the axis of time or space result in a much better estimation of the sampling noise. Just as exponential waiting times are, as you surely are aware of, tightly linked to continuous time, discrete state Markov models, the other TTDD models could be linked to non-Markovian processes of moving/singing/behavior.  In many instances, multiple observations in the same ``point-count" are available. Anyways, this are just some thoughts rapidly put together, please comment on this
if you think it's worthwhile.

\begin{quote}
The reviewer's comments are thought-provoking but ultimately lead to a set of analytical strategies that are different than those we investigate in our manuscript.

Within the world of survey sampling, one of the better known Markov models is \citet{Borchers2013}.
They model marine mammal surveys, where the biological states correspond to depth zones of diving, and only mammals in the surface state are detectable.
One strength of their approach is how easily it handles transitions between behavioral states during the survey.
For non-constant detection in avian point-count surveys, Markov models appear particularly well-suited to account for bout singing --- i.e., when a bird alternates between periods of calling and silence.
Our parametric survival analysis only handles bout singing marginally.

However, for other forms of non-constant detection (e.g. observer effects), it is not clear that Markov models offer an advantage.
A Markov model would feature presumably at least three classes (unavailable, available, and detected), with the chief focus of inference being estimation of the time-dependent transition matrix.
Assumptions about the form of that matrix would pose similar modeling challenges to those we faced in parametric survival analysis.
Markov models do enable the incorporation of supplementary behavioral observations --- for example, \citet{Borchers2013} use data from radio-tagged whales to estimate transition matrices --- but we have no such data to supplement our avian counts.
In one regard, the Markov model is a little more inelegant in that it requires discrete-time observations, whereas the parametric survival analysis in our presentation can utilize exact time-to-detection data.

We reiterate: the comments about Markov models are thought-provoking but are ultimately something different than what we examine in this manuscript.
\end{quote}


\bibliography{masterbib}
\bibliographystyle{biom}

\end{document}
