\documentclass[12pt]{article}
\usepackage[margin=1.0in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{natbib}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\usepackage[margin=1cm]{caption}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{xr}

\renewenvironment{quote}  % Define 
              {\list{}{\rightmargin\leftmargin}\normalfont%
               \item\relax}
              {\endlist}


\externaldocument[M-]{Draft}

%\doublespacing

\newenvironment{itquote}
{\begin{quote}\itshape}
{\end{quote}}

\newcommand{\adam}[1]{{\color{blue} ADAM: #1}}
\newcommand{\jarad}[1]{{\color{Orange} JARAD: #1}}

\newenvironment{indpar}[1]%
     {\begin{list}{}%
             {\setlength{\leftmargin}{\#1}}%
             \item[]%
     }
     {\end{list}}

\newcommand{\vn}{\textbf{n}}
\newcommand{\vp}{\textbf{p}}
\newcommand{\vX}{\textbf{X}}
\newcommand{\vZ}{\textbf{Z}}
\newcommand{\vbeta}{\boldsymbol{\beta}}
\newcommand{\vxi}{\boldsymbol{\xi}}

\newcommand{\Exp}{\mbox{Exp}}
\newcommand{\Ga}{\mbox{Ga}}
\newcommand{\We}{\mbox{We}}
\newcommand{\LN}{\mbox{LN}}
\newcommand{\Po}{\mbox{Po}}
\newcommand{\Mult}{\mbox{Mult}}

\newcommand{\pdet}{p^{(det)}}
\newcommand{\ind}{\stackrel{ind}{\sim}}
\newcommand{\Fm}{F_T^{(M)}}

\begin{document}
\itshape
Associate Editor:

I enjoyed reading this well-written manuscript, and believe that the modeling framework proposed is sufficiently versatile to be used in a wide range of applications. In addition to the points made by the referees, I have two comments:
\begin{enumerate}
\item Reviewer 1 is not convinced that it is helpful to make explicit the link to survival analysis. I tend to disagree with this, since I think that this is what renders the approach widely applicable and also extendable. In fact, the general idea strongly reminded me of several papers published recently by David Borchers and colleagues (using concepts from survival analysis to model detection events within distance sampling and spatially explicit capture-recapture). I was wondering if there is a common denominator between his work and the present manuscript (but the authors need not necessarily explore this).
\begin{quote}
We agree, this manuscript has strong parallels to the continuous-time models of David Borchers and colleagues.  
Interestingly, they have approached removal modeling both from distributional and from hidden Markov modeling directions.
Our manuscript explores distinct topics in that our model is for removal-only data, whereas their models are for removal-distance data.
Also, we explicitly investigate non-constant detection distributions, which has not been their focus.
\end{quote}


\item In Section 3.4, models are built where the various TTDDs considered are linked to covariates via the rate parameter. It was not clear to me why the rate parameter was chosen to be modeled as a function of covariates, as opposed to the mean of the TTDD. For example, in gamma GLMs, the mean is modeled as a function of covariates, with the shape parameter fixed (the latter as done also in the present work). Basically I'm now wondering a) why the rate rather than the mean was modeled, and b) if that matters at all. In 3.1, a justification seems to be given, but I do not follow the reasoning here â€“ why would the inclusion of random and fixed effects be any different if not the rate but the mean would be used? Sorry if I'm just missing the obvious.
\begin{quote}
We have added the following in Section \ref{M-sec:exact_time} to clarify: ``We employ a log link for $\varphi$, and therefore our model is equivalent to a generalized linear model with a log link on the mean detection time."

We feel the rate conceptualization resonates more with the framing of the problem, though perhaps the distinction is mainly aesthetic.
Our view might be more explicit if we were to define the time-to-detection distribution from its hazard function rather than from a traditional distributional family. 
The latter is more convenient and builds upon parametric survival modeling, but the former can also be done within our model framework.
\end{quote}
\end{enumerate}

\newpage



Reviewer \#1:

I have included several items for the authors to consider below and provide this review as an Ecologist and potential model user.
Several pieces of information are lacking from the manuscript that will be relevant to users of this method.
\begin{enumerate}
\item What types of data are relevant to each examined distribution (e.g., gamma, Weibull)?
\item What sample sizes (i.e., number of detections) are needed to obtain reliable estimates with CIs that are not so wide as to be meaningless?
\item What are approximate computation times for models of different complexity (e.g., days? weeks?).
\end{enumerate}

Abstract

Line 14: Perhaps instead of saying its not justified, mention it can lead to bias
\begin{quote}
We have revised the statement to read: ``The standard model for estimating detection from removal-sampled point-count surveys assumes that organisms at a survey site are detected at a constant rate; however, this assumption can often lead to biased estimates.''
\end{quote}

Line 29-31: Clarify that you conducted a simulation analysis and your results from that analysis suggest it can outperform non-mixture models.
\begin{quote}
We have prefaced the results with the clause: ``Based on simulation analyses, ..."
\end{quote}

Line 40-41.  Also worth mentioning caveats - e.g., greater imprecision and computation times.
\begin{quote}
\adam{Address this point}
\end{quote}

Introduction\\
Line 36-41: I find mention of survival analyses here and later in the manuscript confusing for most readers - especially Ecologists, presuming this is the intended audience.  It may be due to the unorthodox use of time-to-detection instead of traditional time-to-event analyses often used to estimate survival.  I suggest either omitting the reference to survival analysis here and elsewhere in the manuscript, or using the time-to-event terminology in this sentence and providing citations for the cdf and pdfs outlined here for reference.  Then explain that within the manuscript you will subsequently refer to it as TTDD.  
\begin{quote}
We have made the equivalence more explicit: ``We analyze times to first detection as time-to-event data, as is done in parametric survival analysis, ..."
\adam{I am unclear what the reviewer means by `providing citations for the cdf and pdfs outlined here for reference'.}
\end{quote}

3.1 \\
Line 60: In the context of estimating abundance of wildlife, justifying the inclusion of the Weibull and lognormal distributions because they are often used in survival analysis is weak.  Are there biologically plausible scenarios in which these distributions are appropriate given this type of data?  Justify here similar to the Gamma distribution.  Otherwise, omit.
\begin{quote}
We have added the following sentences to \ref{M-sec:exact_time}: ``Like the gamma TTDD, Weibull and lognormal TTDDs offer the flexibility of a two-parameter form and allow rates to either increase or decrease during the survey; however, the shapes of these distributions differ, most notably in tail probability.
All three could be reasonable TTDD choices if, for instance, detection rates vary among individuals.
This leads to marginal detection rates that decrease over time as individuals with higher detection rates are removed from the population."

We appreciate the importance of providing biological justification for statistical assumptions.
It is always a good foundation.
That being said, the use of any time-to-detection distribution is a statistical approximation to empirical patterns.
The dynamics of the system may be more complicated than we can appreciate.
For instance, if the arrival of the observer causes non-constant detection, we know neither how long that observer effects lasts nor how the degree of that effect changes over time.
\end{quote}

Simulation Analyses\\
Line 57-59: This is the first mention of peaked and non-peaked TTDDs.  Please define these terms and how they are manifested during point-count surveys in the Introduction - including a biological justification that warrants their inclusion in simulation studies.
\begin{quote}
We have given the definitions of peaked/nonpeaked their own paragraph in the Simulation Studies methodology (Section \ref{M-sec:sim}):
``In the following analyses we distinguish two categories of purely continuous TTDDs: peaked and nonpeaked.  
Detection rates $h(t)$ of peaked distributions generally decrease over time, while detection rates of nonpeaked distributions generally increase over time.
More formally, we define a peaked TTDD as having a mode greater than zero (or $C_1$ for lognormal) while a non-peaked TTDD has a mode of zero (or less than $C_1$), but we consider exponential TTDDs to be neither peaked nor nonpeaked."
We do not believe this distinction requires a separate explanation in the Introduction.
\jarad{Can we reword to remove peaked and nonpeaked here?}
\end{quote}

As a potential user of such a model presented in this manuscript, I find the simulation analyses somewhat lacking.   
\begin{enumerate}
\item Users will most certainly not bother using such complex modeling approach to model an intercept only - and thus, evaluating TTDDs based on intercept-only models seems overly simple and not very informative. \jarad{Computation time.}
\begin{quote}
We agree that an intercept-only model is simplistic and will not be of much practical use in applied settings.
However, it is informative.
TTDDs that are biased (e.g. non-mixture lognormal) or not robust to misspecification (e.g. mixture exponential) may not be trustworthy in more complicated analyses.

On the practical side, while simulation studies are hampered by long computation times, the typical user of this model will only need to apply it to a comparatively smaller number of datasets.
\end{quote}
\item Simulations incorporating variable sample sizes to evaluate relative bias and precision would be very useful in terms of study design and applicability.  
\begin{quote}
\adam{Investigate more/different simulations}
\end{quote}

\item How might study design impact bias and precision?  For example, length of survey relative to peaked TTDDs and pooling of time periods during data collection?  These are also scenarios that would benefit readers.  Alternatively, these issues could be addressed in the Discussion.
\begin{quote}
We appreciate that these are important decision in the design of surveys, but we have not investigated either of these issues with respect to bias and precision.
Not having done so, we prefer not to speculate.
%\citet{Petit1995, Johnson2008, LeeMarsden2008} and \citet{Reidy2011} all address the effects of duration.
We have separated out a paragraph in the Discussion posing some design-related questions.
\adam{Insert a statement about the NRRI one-minute intervals relative to two- or three-interval designs?}
\end{quote}

\item Providing computation times for scenarios that vary by model complexity and sample size (including the computer used for analyses) may also provide a gauge for users as to whether this method is feasible for their needs.
\begin{quote}
\adam{Because of improvements in Stan, I want to run more simulation reps before quantifying this.}
\end{quote}

\item I donâ€™t fully appreciate the need for both 50\% and 90\% coverage estimates.  It seems one estimate would be sufficient, and typically 90\% or 95\% credible intervals are used with these types of analyses?
\begin{quote}
We have retained just the 50\% coverage.
\adam{Waiting until all new simulations are run.}
\end{quote}
\end{enumerate}

Results\\
Page 18, Figure 2:  Holy moly, the CIs of the mixture distributions are exceedingly wide given a sample size of 381 detections (which is a lot relative to many studies).
\begin{quote}
Clarification: 947 detections at 381 sites (which is \textit{really} a lot of detections).
We have placed this information in Section \ref{M-sec:data}: ``The data included 65 sites and a total of 381 surveys with site specific variables including site age,  stock density, and an indicator of select-/partial-cut logging during the 1990s. 
A total of 947 Ovenbirds were counted with a maximum of eight in any single survey."
\end{quote}

For example, The Gamma distribution, touted as the most accurate based on the simulation study, suggests 95\% CIs of p ranges from ~ 0.3 to 1?  And subsequently CIs around N are from ~1.75 to 3.25 so we can say there are somewhere between 56 and 1,778  birds?
Is that a useful estimate?  There is a tradeoff between accuracy and precision happening here, and although precise, biased estimates are not what we want, any alternative needs to provide precision at levels that are still informative.  This is a limitation not sufficiently addressed in the Discussion.
\begin{quote} 
We think our figure was unclear.
The cited credible intervals were for \textit{uncounted} birds not total abundance.
We have now changed the figure to $\log_{10}$(Abundance) to avoid confusion.
The gamma TTDD 95\% credible interval for total abundance is (1007, 2836).
This two- to three-fold scale of uncertainty is commensurate with other studies cited in this manuscript \citep{Diefenbach2007, Reidy2011, Solymos2013, Amundson2014}.
\end{quote}

Discussion\\
Page 21, Lines 12-16:  I do not agree with this assessment.  I think these models are appropriate for a particular subset of data - single species discrete-count surveys with a large sample size and relatively low availability (i.e., whether the animal provides a cue to an observer during the survey)   
\begin{quote}
\adam{What is a `discrete-count survey'?  I have not found a clear reference.\\
I accede the `large sample' point.  But based on our findings, I'd advocate against modeling abundance/perception from any small dataset, because analysis requires either a strong assumption about detection rates or a strong prior.  Any comment on this point would be strengthened if we ran small-dataset simulations.\\
I think the low-availability stipulation is redundant.  If availability is really high, then a removal analysis is not necessary at all.}
\jarad{But how do you know availability is high?}
\end{quote}
where non-constant detection through time is suspected based on the behavioral ecology of the species.
\begin{quote}
By the precautionary principle, we believe the burden of proof should rest with the analyst who wishes to assume constant detection rather than with the analyst who wishes to account for non-constant detection.
This is a central point in our manuscript: the constant-rate assumption should not be the default.
\end{quote}


As the authorsâ€™ suggest, most point-count studies are designed to maximize availability, which is typically high for birds (e.g., 80-95\%) with perceptibility (e.g., the probability an observer detects a cue that is given by an animal) being the much larger source of bias.  Therefore, this model, without including a supplemental method to estimate perceptibility, is of limited use to many readers.
\begin{quote}
We agree.
This manuscript applies to removal-only analyses.
Still, the concept should carry over to removal-distance analyses.
The implementation will be more complicated and will vary depending on how removal- and distance-sampling are integrated.
\end{quote}
Further, although the method may be more accurate even for constant detection data, the loss in precision is such that users will likely not opt to use it unless they suspect non-constant detection through time.  Further, given its presumably arduous computation times (albeit unknown), adding a perceptibility component to the model would likely greatly increase computational demands with unknown implications to bias and precision.  Including a paragraph in the Discussion that outlines when the model would be most beneficial to users (e.g., what type of data, surveys, or sample sizes) and explicitly stating drawbacks (e.g., computation time, loss of precision) would greatly improve this section.  
\begin{quote}
\adam{This last recommendation seems feasible, though I think our final paragraph already addesses some of these issues.}
\jarad{If we already address it, then you should just quote from the manuscript.}
\end{quote}
\vspace{1.5cm}

\newpage



Reviewer \#2:

Although many statistical solutions to known issues in abundance estimation have been proposed to date, a plethora of relevant problems remain untreated.  In this manuscript, the author(s?) put the finger on the wound, so to speak, and deal with a very relevant problem: allowing for heterogeneity in detection, or a non-constant detection rate during animal point-count surveys.  This topic is relevant because, as the authors clearly state multiple times, the statistical properties of abundance estimates are sensitive to the statistical properties of the estimates of the detection probabilities. In that sense, I commend the manuscript's pertinence: the question treated is indeed relevant and needs careful examination.
The advent of computer intensive approaches to estimate parameters in hierarchical models very quickly re-shaped the field of abundance estimation, and in less than a decade, we've moved from trying to explain a complicated natural signal with the simplest possible model to trying to explain a complicated natural signal with a `realistic' but equally complicated sampling model. Then, the burden of the quality of the estimation is put into the process of the specification of the sampling model. However, there is little guarantee that the data, as considered in the paper, contains the necessary information to be able to reliably tease apart all the components of the statistical sampling model. 
\begin{quote}
We appreciate the reviewers' insights and whole-heartedly agree.
\end{quote}
In what follows, I not only expand on this topic but I outline a few major and minor comments that I hope the author(s) will regard as useful to improve the quality of the manuscript. I will certainly recommend this manuscript for publication after the author(s) include the
modifications I request or she (he) successfully convinces me otherwise.



1. MAJOR COMMENTS


Diagnostics: Coverage. The idea of testing coverage is fantastic, but poorly implemented. First, the number of simulations (16 in Table 1) is exceedingly low to be able to reliably diagnose the patterns in coverage or the statistical explanation of these patterns. 
\begin{quote}
We have not been clear enough in stating our goals for the simulations.
We have added the following line in Section \ref{M-sec:sim}: ``Because our main goal with these studies was to demonstrate poor performance under model misspecification, and because the computation time for sampling from some models was large, we simulated only a small number of replicates."
\adam{Can we run more simulations?  Reviewer 1 wants more variation in $n^{(obs)}$.  Reviewer 2 wants a wider range of simulated $\pdet$.}
\end{quote}

Second, the statistical properties of the estimator of p(det) are likely depending on the size of the true value of p(det). Therefore, I think Table 1 should have been repeated for a whole range of values of the true value of p(det)
\begin{quote}
\adam{We should run more simulations.  Reviewer 1 wants more variation in $n^{(obs)}$.  Reviewer 2 wants a wider range of simulated $\pdet$.}
\end{quote}

(Is a boundary like the one in Olkin et al 1981, for your case, a hard one such that below it estimation is bad, and above it estimation is ``uniformly" good?).
\begin{quote}
It is not a hard boundary.  
But the boundary is a property of estimating $N$ in Binomial unknown-$N$, unknown-$p$ problems.
Olkin's calculation results from the relative sample mean and sample variance of large samples.  
Similar thresholds continue to recur in various abundance-estimating contexts, as demonstrated in these recent articles: 
\begin{itemize}
\item \citet{Veech2016}: advise caution when $p<0.5$ in the context of individual-level detection effects (no time-to-detection data). 
\item \citet{Field2016}: plots indicate uncertainty in $\hat{N}$ increase rapidly as $\pdet$ decreases below 0.40.  Methods include removal, double-observer, distance, and multiple-visit.
\item \citet{Davis2016}: report accurate $\hat{N}$ at $\pdet >0.40$ in a traditional removal-sampling context, though for small abundance ($N<50$) they required $\pdet > 0.7$.
\end{itemize}
\adam{Since I've cited the above articles, is there a reason to work them into the manuscript?  I don't feel they add to our story.}
\end{quote}

 Without variation in ``true" simulated scenarios, it is easy to inadvertendly ``stack the deck" in favor or against a particular combination of settings (Particularly since your ``true" value of p is high: 0.8). A wide range of simulated truths is also necessary because to be useful, these methods should be widely applicable in the
tropics (low N's, low p's very often) as well as in temperate forests (large N's, large p's), for example.

\jarad{Although we should look at more simulations. The purpose of the current values
was to give some credence to the Ovenbird analysis.}

Diagnostics: relation of the target parameter to other parameters in the models. Precisely because, as the authors mention, the statistical properties of the estimator of p(det) are tightly linked with the statistical properties of the abundance estimator, then the author(s) should detail explicitly how a bad coverage in p(det), for instance, affects the estimation of abundances. The author(s) mention that bias in p translates into a bias in abundance estimation, I would like to see diagnostics for the realized N values too. And to do that, two words come to mind: profile likelihoods. Unless you are explicitly adopting a subjective Bayesian approach, you basically declare a priori ignorance for your parameters.  Furthermore, you state that typically you will be in a case where data sets aren't large, so the information in the data is not ``swamping" the priors, so to speak.
\begin{quote}
We have posterior estimates of abundance from our model fits.  We have added them to the manuscript.\\
\adam{Thoughts on how to do this:
\begin{enumerate}
\item Generate Tables 1 \& 2 for $\hat{N}$ as well as $\pdet$, but this gets crowded.
\item Supplement Tables 1 \& 2 with some manner of confidence intervals for $\hat{N} / N$, but averaging across replicates is not a trivial issue
\item Supplement Figure 1 with a row of representative abundance posteriors (no averaging required)
\item Following the profile likelihood comment, generate a plot of $\hat{N}$ vs. $\pdet$ for a representative sample.
\end{enumerate}
}
\jarad{I think 3.}
\end{quote}

Lele et al 2010 (a frequent co-author of Solymos) propose a pretty neat diagnostic tool to assess
estimability that is the by product of tricking a bayesian MCMC set up into Maximum Likelihood estimation for hierarchical models (see Lele et al 2007, 2010 and others. The keyword is ``Data Cloning"). You are one step away from using Solymos and Lele's ``Data Cloning" (DC) approach to get the full ML estimation working (See Lele et al 2010 and other Data Cloning papers). Now, this is relevant because using DC you can provide clear estimability diagnostics for the parameters of interest.  Are there parameters that are technically not estimable?  Given that you don't have informative priors, you are ``driving in the dark" so to speak if you are not certain if some of your parameters are un-identifiable (see Lele et al 2010 and discussion in Lele and Dennis 2009).  Note that I am NOT asking you to re-do all the analysis using a ML approach, I am just suggesting the fact that using DC to get the ML estimates one can, as a very useful by product, get very neat diagnostics regarding
the estimability of your parameters.  Again, see Lele et al 2010.  Implementing DC would only imply a simple modification to the programs you already have. And by the way, Solymos has a DC package easy to use.
\begin{quote}
\adam{
\begin{itemize}
\item The Solymos package is built for BUGS and JAGS.  Most of the tools are easily hand-coded, I think.
\item To simplify our lives, we could choose to attempt this for only quick-running models on the actual data... there's no reason to suspect identifiability issues for the gamma TTDD that would not appear for the Weibull or lognormal.
\item Technical note: they suggest using narrower priors as the number of clones increases.  Likewise, we could input posterior means as initial values.
\end{itemize}
}
\jarad{I'm not inclined to do this.}
\end{quote}

Generalized Pareto: In another area in biology, in evolutionary genetics, the ``fitness" of individual variant strains has been modeled as coming from an exponential distribution.  For various reasons having to do with the biology of the system, such model was quickly challenged and pretty soon papers questioning the validity of this or this other model for data akin to your waiting times abounded. There is one pretty interesting paper by Beisel et al, 2007 (Genetics, 10.1534/genetics.106.068585), written towards the epiloge of such discussions, that posits that a particular parameterization of the Generalized Pareto distribution, with a single stroke, encompassed many suitable probabilistic models in the Weibull, Gumbel and Freched domain of attraction.  I wonder if the authors could write a general parameterization so that changing from one distribution to the other would simply amount to dialing a given parameter, much like Beisel et al do?  This would be just a practical
consideration that can at once deal with model selection and could speed up calculations
\begin{quote}
We appreciate the merit of the reviewer's suggestion and have added comments in the Discussion.
We focus our comments on a possible generalized gamma TTDD, which is an umbrella distribution that includes exponential, gamma, lognormal, and Weibull distributions as special cases.
The generalized Pareto analysis in \citet{Beisel2007} requires observations from the tail of the TTDD distribution, which we do not have.
Additionally, its focus is on flexible domains of attraction, whereas the TTDDs in our analysis are all from the Gumbel domain of attraction.

Here are the comments we have added:
``Just as the exponential TTDD is a special case of the two-parameter gamma and Weibull TTDDs, so all four TTDDs in our analysis are special cases the three-parameter generalized gamma distribution.
A generalized gamma TTDD encompasses a diversity hazard functions \citep{Cox2007}, eliminating the need to restrict analysis to lognormal, gamma, or Weibull TTDDs and the tail probabilities they imply.
However, maximum likelihood estimation of the generalized gamma has historically suffered from computational difficulties, unsatisifactory asymptotic normality at large sample sizes, and non-unique roots to the likelihood equation \citep{CoorayAnand2008, NoufailyJones2013}.
It may well be possible to implement our model with a generalized gamma TTDD, but especially when we consider the right-truncation of data from point-count surveys, we think model convergence would not be a trivial problem."
\adam{This is an area of active research.  I feel like my tone is not optimistic enough.  Then again, it's not worth micro-wordsmithing.}
\end{quote}

Writing: The multiplicity of mathematical explicit meanings for the word ``mixture" may result confusing. The distinction between the mixture component a la Farnsworth et al and the N-mixture models (e.g. binomial counts with N~pois and p random) is clear to me, but it may not be for some of JABES' audience.  I suggest re-shaping the introduction to that effect. 
\begin{quote}
In the Introduction, we now clearly describe a practice of modeling the TTDD ``as a mixture of two distributions --- a continuous-time distribution and a point mass for increased detection probability in the initial observation period."
Later in the same paragraph, we state ``We apply the term `mixture model' to any model with a mixture TTDD."
This is contrasted in the next paragraph: ``a hierarchical framework for multinomial counts called an N-mixture model \dotso, which is an entirely different use of `mixture' from the mixture models in the previous paragraph."
\end{quote}

One thing that the authors could do is to take the reader by the hand by building equation 1 bit by bit: present first the simplest version of equation 1 and what is customary to date, and then add modifications/hierarchies plus text and build the model with explicit equations and little by little arrive to the final product, which is the current equation 1. Nothing better than clear simple math aided by short explanations to present a model unambiguously.
\begin{quote}
\adam{
This potentially requires a lot of rewiring.  Two options are: (i) highlight each component of Equation (1) as we address it in the text, because we do address them all, or (ii) at the end of Section 3.2, provide a simpler summary of the model without mixtures and without covariates.
I do think we provide the model pieces one-by-one, but we could be more heavy-handed (in a good way).
}
\jarad{But it appears the reviewer wants it up front. 
I need to go back and reread to determine if this seems feasible. 
I thought we did start without mixtures and without covariates.}
\end{quote}

Write self-contained paragraphs, with one main idea, not two or three (and each one half developed). As a reviewer, I enjoyed verifying that the cited papers in the introduction and in the discussion were indeed meaningful (by downloading them and reading those that I was not familiar with). However, when many papers are cited, it is useful to expand in the text why the different papers are being cited.  Doing so not only clarifies your intentions, but does proper justice to the papers being referred to. It is also a useful exercise because it allows you to tell whether you have more than one clear ideas and hence, material for more than one paragraph.  For instance, I think that the paragraph in the introduction, line 34, page 3, could be expanded and/or broken into two paragraphs.  One introducing the time-to-detection as is done in survival analysis, and one presenting to the reader how data that seem not to conform to the constant-detection assumption is dealt with by
using the idea of the increased detection probability via a mixture component. These are two different ideas, hence two different paragraphs. The same approach to construct paragraphs through the text should be taken.
\begin{quote}
We have reviewed the entire manuscript with these points in mind.
We have split the cited paragraph and a few others.
\adam{The only times I feel I have cited many articles en masse is when I've said ``Many authors have done this.'}
\end{quote}

The authors could have crafted a beautiful and very telling graph (Figure 1) representing the multinomial intervals, the TTDD, the right censoring due to the end of the observation period, and the increased detection probability mixture component. Such figure (which is what I did by hand as I was reading the manuscript) would be a very useful guide in the reading of the paper.
\begin{quote}
We have added this as Figure \ref{M-fig:schematic}.
\adam{I have not cited the figure in the text.  I am not convinced I need to.}
\end{quote}


Process vs. observation error: Finally, here's a perspective outside the author's topic, but within statistical ecology that might result in a markedly improvement of these ``N-mixture" techniques, along with all of its variants. I wonder if thinking a bit more in terms of modeling the biological process behind the data can result in a better estimation of the sampling variance and as a ``by-product", some novel understanding of the biology behind the data. I suppose the author(s) are familiar (or at least have seen) the multiple papers where statistical inference is done for a stochastic population dynamics model using time series of abundances while taking into account sampling error. In these settings, it is customary to deal with one observed abundance per time step. The sampling error and the ecologically-phrased variability are phrased using a hierarchical model.  When statistical inference is done for this hieararchical model, the information to be able to tease apart
the process from the observation variance lies within the structure of the temporal dependencies phrased in the model. Amazingly, with a single observation per time step, the sampling model is not ill posed as the time dependencies in the process brings about enough information in the data.  As a result, thinking of the biological process (in that case, the population dynamics model) yields better, unbiased estimates of the observation error. In your case, I cannot help but wonder if having data varying along the axis of time or space result in a much better estimation of the sampling noise. Just as exponential waiting times are, as you surely are aware of, tightly linked to continuous time, discrete state Markov models, the other TTDD models could be linked to non-Markovian processes of moving/singing/behavior.  In many instances, multiple observations in the same ``point-count" are available. Anyways, this are just some thoughts rapidly put together, please comment on this
if you think it's worthwhile.

\begin{quote}
\adam{
The main application I've encountered before is \citet{Borchers2013}.
There are technical differences between that approach and our parameteric survival analysis approach, mainly due to the continuous and interval-censored handling of time.
For instance, I suspect that for analysis, as with \citet{Farnsworth2002} and its derivatives, the hidden Markov model would require an artificial division of the observation period into equal-intervals intervals.
One cool feature of that approach is that it directly handles bout-singing through an availability state (which may be stochastically `available' or `not available') in conjunction with a detection model.
I am not sure that model would serve well for a problem like ours where we are looking for systematic non-constant detection --- i.e., the Markov model would require time-dependent transition matrices.
Additionally, at least in the case of \citet{Borchers2013}, the model utilizes separate observations of animal behaviors to estimate transitions among hidden states (e.g., moving/singing/etc.).
In their case, they analyzed radio-tagged whales to estimate transitions for diving states.

Those are just some preliminary thoughts.
I think the reviewer's comment is thought-provoking but not applicable to the manuscript we have crafted.}
\end{quote}


\bibliography{masterbib}
\bibliographystyle{biom}

\end{document}
